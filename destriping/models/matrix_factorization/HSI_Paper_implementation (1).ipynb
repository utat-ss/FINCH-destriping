{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2933282f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='svg'"
      ],
      "id": "2933282f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0b246422"
      },
      "outputs": [],
      "source": [
        "# M, N, L = 180, 180, 1   # these are the values mentioned in the paper\n",
        "M, N, L = 145, 145, 200\n",
        "# M = height of HSI\n",
        "# N = width of HSI\n",
        "# L = number of spectral bands in HSI"
      ],
      "id": "0b246422"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82a6e846",
        "outputId": "8f576ead-7d56-47e8-8900-126e0f16f67e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X's shape: (200, 21025)\n",
            "Y's shape: (200, 21025)\n"
          ]
        }
      ],
      "source": [
        "# original image\n",
        "X = np.random.randn(L, M*N)\n",
        "\n",
        "# White Gaussian noise\n",
        "mu = 0     # mean\n",
        "sigma = 1  # standard deviation\n",
        "white_gaussian_noise = np.random.normal(mu, sigma**2, size=(L, M*N))\n",
        "\n",
        "# Sparse noise (drawn from zero-mean Laplace distribution)\n",
        "sparse_noise = np.random.laplace(loc=0, scale=1, size=(L, M*N))\n",
        "\n",
        "# Y = X + N + S\n",
        "Y = X + white_gaussian_noise + sparse_noise\n",
        "\n",
        "# note both X and Y are from the standard normal distribution\n",
        "\n",
        "print(f\"X's shape: {X.shape}\")\n",
        "print(f\"Y's shape: {Y.shape}\")"
      ],
      "id": "82a6e846"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do not plot X and Y as they hang your laptop.**"
      ],
      "metadata": {
        "id": "h78YUSIWnAa-"
      },
      "id": "h78YUSIWnAa-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "090362e8"
      },
      "source": [
        "Information in this cell, **do not delete**\n",
        "\n",
        "<!-- # Ignore this... (just for demonstrating MLE)\n",
        "\n",
        "Example showing maximum likelihood estimation. But the authors said that it is ill posed.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Generate some sample data\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(loc=5, scale=2, size=100)\n",
        "\n",
        "# Define the likelihood function for a normal distribution\n",
        "def normal_likelihood(data, mu, sigma):\n",
        "    n = len(data)\n",
        "    likelihood = (1/(np.sqrt(2*np.pi*sigma**2)))**n * np.exp(-1/(2*sigma**2) * np.sum((data-mu)**2))\n",
        "    return likelihood\n",
        "\n",
        "# Define the initial guesses for mu and sigma\n",
        "mu_0 = 0\n",
        "sigma_0 = 1\n",
        "\n",
        "# Define the tolerance level for convergence\n",
        "tol = 0.01\n",
        "\n",
        "# Initialize the parameters\n",
        "mu = mu_0\n",
        "sigma = sigma_0\n",
        "\n",
        "# Initialize the change in parameters\n",
        "change = tol + 1\n",
        "\n",
        "# Define the learning rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Perform the maximum likelihood estimation\n",
        "while change > tol:\n",
        "    # Compute the likelihood\n",
        "    likelihood = normal_likelihood(data, mu, sigma)\n",
        "    \n",
        "    # Compute the gradient of the likelihood function with respect to mu and sigma\n",
        "    grad_mu = np.sum(data-mu)/sigma**2\n",
        "    grad_sigma = -n/(sigma) + np.sum((data-mu)**2)/sigma**3\n",
        "    \n",
        "    # Update the parameters\n",
        "    mu_new = mu + learning_rate*grad_mu\n",
        "    sigma_new = sigma + learning_rate*grad_sigma\n",
        "    \n",
        "    # Compute the change in parameters\n",
        "    change = np.abs(mu_new - mu) + np.abs(sigma_new - sigma)\n",
        "    \n",
        "    # Update the parameters\n",
        "    mu = mu_new\n",
        "    sigma = sigma_new\n",
        "\n",
        "# Print the estimated parameters\n",
        "print(\"Estimated mu:\", mu)\n",
        "print(\"Estimated sigma:\", sigma)\n",
        "``` -->"
      ],
      "id": "090362e8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About LSMM, Abundance matrix, Endmember matrix\n",
        "\n",
        "In the Linear Spectral Mixing Model (LSMM), an abundance matrix and an endmember matrix are two important components used to represent and analyze the multi-spectral image data.\n",
        "\n",
        "- An endmember matrix is a matrix of pure spectral signatures representing the different materials present in the scene. Each column in the endmember matrix corresponds to a single endmember and each row corresponds to a single wavelength or band of the electromagnetic spectrum. The endmember matrix is usually obtained by manually selecting a set of pixels that are known to correspond to a specific material, and then averaging their spectral signatures.\n",
        "\n",
        "- An abundance matrix is a matrix of mixing coefficients, where each coefficient represents the proportion of a specific endmember in a pixel. Each column in the abundance matrix corresponds to a single pixel, and each row corresponds to a single endmember. The abundance matrix is usually obtained by solving a system of linear equations, where the observed spectral pixel vectors are modeled as a linear combination of the true signals (endmembers), and the mixing coefficients are the unknown parameters that need to be estimated.\n",
        "\n",
        "In summary, the endmember matrix provides a set of spectral signatures that represent the different materials present in the scene. While, the abundance matrix provides information about the proportions of these materials in each pixel of the image. \n",
        "\n",
        "Both matrices are essential for understanding the composition of the scene and for performing further analysis and interpretation of the multi-spectral image data."
      ],
      "metadata": {
        "id": "-gKPTS0Vskcu"
      },
      "id": "-gKPTS0Vskcu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSMM (Linear Spectral Mixing Model) implementation\n",
        "\n",
        "1. Load the multi-spectral image data: You can use libraries such as GDAL or rasterio to read the image data into a numpy array.\n",
        "\n",
        "2. Extract the spectral pixel vectors: For each pixel in the image, extract the reflectance or radiance values for each band and store them as a spectral pixel vector. This can be done using numpy array slicing and indexing.\n",
        "\n",
        "3. Estimate the mixing coefficients and true signals: There are several algorithms and methods that can be used to perform this estimation, such as linear unmixing, linear regression, and principal component analysis. You can use libraries such as scikit-learn to implement these algorithms.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Linear unmixing algorithm\n",
        "def linear_unmixing(data, endmembers):\n",
        "    \"\"\"\n",
        "    data: (N, M) array, where N is the number of pixels and M is the number of bands\n",
        "    endmembers: (K, M) array, where K is the number of endmembers and M is the number of bands\n",
        "    \"\"\"\n",
        "    model = LinearRegression()\n",
        "    model.fit(endmembers, data)\n",
        "    return model.coef_, model.intercept_\n",
        "\n",
        "# Extract the spectral pixel vectors\n",
        "data = image.reshape(-1, n_bands)\n",
        "\n",
        "# Estimate the mixing coefficients and true signals\n",
        "coef, intercept = linear_unmixing(data, endmembers)\n",
        "```"
      ],
      "metadata": {
        "id": "no6G3yTrocLD"
      },
      "id": "no6G3yTrocLD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is assummed in model that `X` can be factorized into 2 non-negative matrices.  "
      ],
      "metadata": {
        "id": "KEVd64LRN32P"
      },
      "id": "KEVd64LRN32P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model in TensorFlow:\n",
        "\n",
        "```python\n",
        "# Input as specified in the paper\n",
        "# M, N, L = 180, 180, 1   # these are the values mentioned in the paper\n",
        "M, N, L = 145, 145, 200\n",
        "input_shape = (1, M, N, L)\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "# Model starts\n",
        "model_input = input_layer\n",
        "# Block 1\n",
        "x = Conv3D(filters=64, kernel_size=(3,3,1), activation='relu', padding='same', strides=1, \n",
        "           dilation_rate=1, name='block1')(model_input)\n",
        "# Block 2\n",
        "x = Conv3D(filters=64, kernel_size=(3,3,64), activation='relu', padding='same', strides=1, \n",
        "           dilation_rate=2, name='block2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "# Block 3\n",
        "x = Conv3D(filters=64, kernel_size=(3,3,64), activation='relu', padding='same', strides=1,\n",
        "           dilation_rate=4, name='block3')(x)\n",
        "x = BatchNormalization()(x)\n",
        "# Block 4\n",
        "x = Conv3D(filters=64, kernel_size=(3,3,64), activation='relu', padding='same', strides=1,\n",
        "           dilation_rate=3, name='block4')(x)\n",
        "x = BatchNormalization()(x)\n",
        "# Block 5\n",
        "x = Conv3D(filters=64, kernel_size=(3,3,64), activation='relu', padding='same', strides=1,\n",
        "           dilation_rate=4, name='block5')(x)\n",
        "x = BatchNormalization()(x)\n",
        "# Block 6\n",
        "x = Conv3D(filters=64, kernel_size=(3,3,64), activation='relu', padding='same', strides=1,\n",
        "           dilation_rate=2, name='block6')(x)\n",
        "x = BatchNormalization()(x)\n",
        "# Block 7\n",
        "x = Conv3D(filters=64, kernel_size=(3,3,64), activation='relu', padding='same', strides=1,\n",
        "           dilation_rate=1, name='block7')(x)\n",
        "# Skip Connection\n",
        "x = Concatenate(axis=-1)([x, input_layer])\n",
        "# x = tf.activation.relu()(x)\n",
        "# x = BatchNormalization()(x)\n",
        "\n",
        "# Block 8 -- (not in the paper) I added this so as to match the \n",
        "# spectral/channels dimension with the dimensions of the original image\n",
        "output = Conv3D(filters=200, kernel_size=(3,3,64), activation='relu', padding='same', strides=1, name='block8')(x)\n",
        "```"
      ],
      "metadata": {
        "id": "cVp2uxwmSkMA"
      },
      "id": "cVp2uxwmSkMA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model in PyTorch:"
      ],
      "metadata": {
        "id": "5sYWkfzySqw-"
      },
      "id": "5sYWkfzySqw-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2F5Y67wk12K"
      },
      "outputs": [],
      "source": [
        "# Network architecture\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, spectral_dim):\n",
        "        \"\"\"\n",
        "        spectral_dim = number of spectral bands or number of channels in the \n",
        "        input image \n",
        "        \"\"\"\n",
        "        super(MyModel, self).__init__()\n",
        "        self.block1 = nn.Conv3d(in_channels=spectral_dim, out_channels=64, \n",
        "                                kernel_size=(3,3,1), stride=1, padding=\"same\", \n",
        "                                dilation=1)\n",
        "        self.block2 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(3,3,64),\n",
        "                                stride=1, padding=\"same\", dilation=2)\n",
        "        self.block3 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(3,3,64),\n",
        "                                stride=1, padding=\"same\", dilation=4)\n",
        "        self.block4 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(3,3,64),\n",
        "                                stride=1, padding=\"same\", dilation=3)\n",
        "        self.block5 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(3,3,64),\n",
        "                                stride=1, padding=\"same\", dilation=4)\n",
        "        self.block6 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(3,3,64),\n",
        "                                stride=1, padding=\"same\", dilation=2)\n",
        "        self.block7 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=(3,3,64),\n",
        "                                stride=1, padding=\"same\", dilation=1)\n",
        "        # block8 is not in the paper - I added it so as to match the spectral/channels \n",
        "        # dimension with the dimensions of the original paper\n",
        "        self.block8 = nn.Conv3d(in_channels=64, out_channels=200, kernel_size=(3,3,64),\n",
        "                                stride=1, padding=\"same\", dilation=1)\n",
        "        self.bnorm_64 = nn.BatchNorm2d(num_features=64)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_copy = x.clone()\n",
        "        x = self.relu(self.block1(x))\n",
        "        x = self.bnorm_64(self.relu(self.block2(x)))\n",
        "        x = self.bnorm_64(self.relu(self.block3(x)))\n",
        "        x = self.bnorm_64(self.relu(self.block4(x)))\n",
        "        x = self.bnorm_64(self.relu(self.block5(x)))\n",
        "        x = self.bnorm_64(self.relu(self.block6(x)))\n",
        "        x = self.relu(self.block7(x))\n",
        "        x = self.relu(x + x_copy)\n",
        "        return x"
      ],
      "id": "H2F5Y67wk12K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CseTaGsgk1qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34fdf41d-fddf-4889-88ec-d139ba6dcaf3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (block1): Conv3d(200, 64, kernel_size=(3, 3, 1), stride=(1, 1, 1), padding=same)\n",
              "  (block2): Conv3d(64, 64, kernel_size=(3, 3, 64), stride=(1, 1, 1), padding=same, dilation=(2, 2, 2))\n",
              "  (block3): Conv3d(64, 64, kernel_size=(3, 3, 64), stride=(1, 1, 1), padding=same, dilation=(4, 4, 4))\n",
              "  (block4): Conv3d(64, 64, kernel_size=(3, 3, 64), stride=(1, 1, 1), padding=same, dilation=(3, 3, 3))\n",
              "  (block5): Conv3d(64, 64, kernel_size=(3, 3, 64), stride=(1, 1, 1), padding=same, dilation=(4, 4, 4))\n",
              "  (block6): Conv3d(64, 64, kernel_size=(3, 3, 64), stride=(1, 1, 1), padding=same, dilation=(2, 2, 2))\n",
              "  (block7): Conv3d(64, 64, kernel_size=(3, 3, 64), stride=(1, 1, 1), padding=same)\n",
              "  (block8): Conv3d(64, 200, kernel_size=(3, 3, 64), stride=(1, 1, 1), padding=same)\n",
              "  (bnorm_64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "model1 = MyModel(spectral_dim=L)\n",
        "model1"
      ],
      "id": "CseTaGsgk1qt"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjwcZqnJnEmQ"
      },
      "id": "QjwcZqnJnEmQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_dim, channels, height, width\n",
        "X_tensor = torch.from_numpy(X).reshape(1, L, M, N)\n",
        "Y_tensor = torch.from_numpy(Y).reshape(1, L, M, N)\n",
        "print(X_tensor.shape, Y_tensor.shape, sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ep1_nfwWw8R",
        "outputId": "d2b2ca8b-2710-4caa-e875-abdd12aefa41"
      },
      "id": "7Ep1_nfwWw8R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 200, 145, 145])\n",
            "torch.Size([1, 200, 145, 145])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # create a new tensor to fit the input requirements of the model\n",
        "# model1(X_tensor)\n",
        "# model_output = model1(X_tensor)"
      ],
      "metadata": {
        "id": "racoLIz2XKaS"
      },
      "id": "racoLIz2XKaS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HSI Denoising with NMF-DPR Algorithm\n",
        "\n",
        "First we initialize E and A, which are endmember and abundance matrices, respectively. Now, there are 5 steps inside a while loop:\n",
        "1. Update V using eqn. 12\n",
        "2. Update A using eqn. 19\n",
        "3. Update S using eqn. 9\n",
        "4. Update U using eqn. 10\n",
        "5. Update E using eqn. 11\n",
        "\n",
        "Output is X = EA, which is denoised image."
      ],
      "metadata": {
        "id": "JrmDC6CBX_9N"
      },
      "id": "JrmDC6CBX_9N"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**initialization E and A**\n",
        "\n",
        "- First use above mentioned model (which uses spatial correlation for image denoising) to get initial denoised image Y\n",
        "- Then use \"A variable splitting augmented Lagrangian approach to linear spectral unmixing\" method (which uses spectral correlation for image denoising) on the basis of this Y to get E and A. This is rough denoising-preprocessing and is not as good as proposed approach but is very time-efficient, so for a start, we'll use this.\n",
        "\n"
      ],
      "metadata": {
        "id": "2OXxTqHceEmC"
      },
      "id": "2OXxTqHceEmC"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def linear_spectral_unmixing(Y, E_init, A_init, lambda_init=0.1, max_iter=100, tol=1e-4):\n",
        "    \"\"\"\n",
        "    Linear spectral unmixing using the variable splitting augmented Lagrangian approach.\n",
        "    :param Y: mixed pixels matrix (m x n)\n",
        "    :param E_init: initial endmembers matrix (m x k)\n",
        "    :param A_init: initial abundance fractions matrix (k x n)\n",
        "    :param lambda_init: initial value of the Lagrange multiplier\n",
        "    :param max_iter: maximum number of iterations\n",
        "    :param tol: tolerance for convergence\n",
        "    :return: estimated endmembers matrix (m x k) and abundance fractions matrix (k x n)\n",
        "    \"\"\"\n",
        "    # Define variables\n",
        "    E = E_init.copy()\n",
        "    A = A_init.copy()\n",
        "    lambda_val = lambda_init\n",
        "\n",
        "    # Start iterations\n",
        "    for i in range(max_iter):\n",
        "        # Update endmembers\n",
        "        E_prev = E.copy()\n",
        "        A_T = A.T\n",
        "        A_T_A = np.dot(A_T, A)\n",
        "        A_T_Y = np.dot(A_T, Y)\n",
        "        E = np.dot(A_T_Y, np.linalg.inv(A_T_A + lambda_val * np.eye(A.shape[0])))\n",
        "\n",
        "        # Update abundance fractions\n",
        "        A_prev = A.copy()\n",
        "        Y_E = np.dot(Y, E.T)\n",
        "        E_E = np.dot(E, E.T)\n",
        "        A = A_prev * (Y_E / (E_E + lambda_val * np.eye(E.shape[1])))\n",
        "        A = A / np.sum(A, axis=0)\n",
        "        A[A < 0] = 0\n",
        "\n",
        "        # Update lambda\n",
        "        lambda_prev = lambda_val\n",
        "        lambda_val = lambda_val * (np.linalg.norm(Y - np.dot(E, A)) / np.linalg.norm(Y - np.dot(E_prev, A_prev)))\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(E - E_prev) < tol and np.linalg.norm(A - A_prev) < tol and abs(lambda_val - lambda_prev) < tol:\n",
        "            break\n",
        "\n",
        "    return E, A"
      ],
      "metadata": {
        "id": "C4Eu2Hz3d6o6"
      },
      "id": "C4Eu2Hz3d6o6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to get initial endmembers and abundance matrix of X, we'll use \n",
        "# Non-negative matrix factorization will decompose X, we assume it to be\n",
        "# non-negative, factor it into the product of two other non-negative matrices, \n",
        "# an endmember matrix and an abundance matrix.\n",
        "\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Number of endmembers\n",
        "n_endmembers = 3\n",
        "\n",
        "# Create an NMF model\n",
        "nmf_model = NMF(n_components=n_endmembers, init='random', random_state=0)\n",
        "\n",
        "# let's create a dummy X matrix\n",
        "demo_X = np.random.rand(2,5)\n",
        "\n",
        "# Fit the model to the data\n",
        "E_init = nmf_model.fit_transform(demo_X)\n",
        "A_init = nmf_model.components_\n",
        "\n",
        "# the product of E and A will give a rough approximate of X"
      ],
      "metadata": {
        "id": "KtiDm1_5llt0"
      },
      "id": "KtiDm1_5llt0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(E_init, A_init, sep='\\n\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QVjRxDVmT1Q",
        "outputId": "db352078-f42a-4fa4-c7a0-b9086d922fc0"
      },
      "id": "0QVjRxDVmT1Q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.30675988 0.         1.18686128]\n",
            " [0.24928935 1.47260379 0.32589397]]\n",
            "\n",
            "\n",
            "[[0.75950885 0.         1.09125601 0.30748747 0.27245271]\n",
            " [0.40922849 0.40536429 0.         0.02090917 0.38203944]\n",
            " [0.00267827 0.47574103 0.54685045 0.09147712 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**condition of while loop**"
      ],
      "metadata": {
        "id": "qjHabAywd-wb"
      },
      "id": "qjHabAywd-wb"
    },
    {
      "cell_type": "code",
      "source": [
        "E_old = nmf_model.fit_transform(model_output)\n",
        "A_old = nmf_model.components_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "TghQOCITlGB0",
        "outputId": "c95bb65f-64a1-4571-c77b-e6f68cd30e86"
      },
      "id": "TghQOCITlGB0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-c2f07084c1ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mE_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mA_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, W, H)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massume_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m             \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m         self.reconstruction_err_ = _beta_divergence(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/decomposition/_nmf.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[0;34m(self, X, y, W, H, update_H)\u001b[0m\n\u001b[1;32m   1582\u001b[0m             \u001b[0mActual\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \"\"\"\n\u001b[0;32m-> 1584\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NMF (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0;31m# check parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to NMF (input X)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def frob_norm(matrix):\n",
        "    \"\"\"\n",
        "    We calculate the frobenius norm, which is simply the \"Euclidean\" distance \n",
        "    between all the elements of a matrix. \n",
        "    It is defined as the square root of the sum of the squares of all elements \n",
        "    in the matrix\n",
        "    \"\"\"\n",
        "    return np.linalg.norm(matrix, \"fro\")\n",
        "    # alternate implementation\n",
        "    # return np.sqrt(np.sum(np.square(matrix)))\n",
        "\n",
        "matrix = A_new*E_new - A_old*E_old\n",
        "T_iter = 1  # assuming this means the number of epochs\n",
        "condition_part1 = frob_norm(matrix) / np.sqrt(L*M*N) < 1e-6\n",
        "condition_part2 = T_iter <= 150\n",
        "condition = condition_part1 and condition_part2"
      ],
      "metadata": {
        "id": "zy5gkrCsX977"
      },
      "id": "zy5gkrCsX977",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**eqn 7**"
      ],
      "metadata": {
        "id": "BVDbBE4yoEee"
      },
      "id": "BVDbBE4yoEee"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# Define the log-posterior function\n",
        "def log_posterior(params, x, y):\n",
        "    intercept, slope = params\n",
        "    sigma = 1  # Assume known constant noise level\n",
        "    log_likelihood = -0.5 * np.sum((y - (intercept + slope * x))**2 / sigma**2 + np.log(2 * np.pi * sigma**2))\n",
        "    log_prior = -0.5 * (intercept**2 + slope**2)\n",
        "    return -(log_likelihood + log_prior)\n",
        "\n",
        "# Define the data\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 3, 4, 5, 6])\n",
        "\n",
        "# Find the maximum of the log-posterior using the minimize function\n",
        "result = minimize(log_posterior, x0=[0, 0], args=(x, y))\n",
        "\n",
        "# Extract the MAP estimates for the intercept and slope\n",
        "intercept_MAP, slope_MAP = result.x\n",
        "\n",
        "print(\"MAP estimates for the intercept and slope:\", intercept_MAP, slope_MAP)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXacnxGJoEgn",
        "outputId": "c727c0c2-401a-4306-b20f-cbc5980fe8d6"
      },
      "id": "GXacnxGJoEgn",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP estimates for the intercept and slope: 0.6306305044599195 1.0810811139869003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we defined the negative logarithm of the posterior function as the function to be minimized, and passed the starting point for the optimization (`[0, 0]`), and the data (`x, y`) as additional arguments to the `minimize` function. The function returns a `OptimizeResult` object, which contains the MAP estimates for the parameters in the `x` attribute.\n",
        "\n",
        "Note that the only difference between the MAP estimation using `minimize` function instead of `maximize` function is the negative sign that we added in the definition of the log-posterior function, since the `minimize` function finds the minimum of a scalar function, while we want to find the maximum of the log-posterior.\n",
        "\n",
        "It's also worth mentioning that Scipy library also offers optimization functions such as `minimize_scalar` that can be used to perform similar optimization tasks with scalar functions.\n",
        "\n",
        "In summary, using the `minimize` function from the `scipy.optimize` module can be a simple way to perform MAP estimation, as it abstracts away some of the implementation details and allows you to focus on defining the log-posterior function."
      ],
      "metadata": {
        "id": "u-42notURHKE"
      },
      "id": "u-42notURHKE"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# define the function to be minimized on E, A, S\n",
        "def some_function(x):\n",
        "    # x[0] is E; x[1] is A; x[2] is S\n",
        "    sigma = 1\n",
        "    T = \n",
        "    return 0.5 * frob_norm(Y - x[0]*x[1]  - x[2])**2 +  \n",
        "\n",
        "# initial guess for E, A, S\n",
        "x0 = np.array([1,2,3])\n",
        "\n",
        "# find the values of E, A, S that give minimum value of some_function\n",
        "minimize(some_function, x0)"
      ],
      "metadata": {
        "id": "ilovOhbSRH0A"
      },
      "id": "ilovOhbSRH0A",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**eqn 8 - alternate way of solving eqn 7**"
      ],
      "metadata": {
        "id": "exANjqlNoGNX"
      },
      "id": "exANjqlNoGNX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ignore this -- see the next code cell**\n",
        "\n",
        "In this example, `A` is the low-rank matrix, and `tau` is the regularization parameter that controls the strength of the shrinkage. The function `shrinkage` first performs a singular value decomposition (SVD) of the low-rank matrix, then shrinks the singular values by subtracting `tau` and setting negative values to zero. Finally, it recompose the matrix by taking the dot product of the left and right singular vectors with the modified singular value matrix.\n",
        "\n",
        "The output of the function `shrinkage` is the shrunk low-rank matrix. This term can be added to the optimization problem of TV-LRTF to improve the restoration results.\n",
        "\n",
        "It's worth mentioning that this is a simple example, in practice the shrinkage operator is often used in conjunction with other regularization terms and methods, such as total variation and sparsity constraints.\n",
        "\n",
        "```python\n",
        "# Define the shrinkage operator\n",
        "def shrinkage(A=np.random.rand(256, 256), tau=0.1):\n",
        "    # A is low-rank matrix. eg: A = np.random.rand(256, 256)\n",
        "    # tau is regularization parameter. eg: tau = 0.1\n",
        "    U, S, V = np.linalg.svd(A, full_matrices=False)\n",
        "    S = np.maximum(S - tau, 0)\n",
        "    return np.dot(U, np.dot(np.diag(S), V))\n",
        "\n",
        "# Define the low-rank matrix\n",
        "A = np.random.rand(256, 256)\n",
        "\n",
        "# Define the regularization parameter\n",
        "tau = 0.1\n",
        "\n",
        "# Apply the shrinkage operator to the low-rank matrix\n",
        "A_shrunk = shrinkage(A, tau)\n",
        "```"
      ],
      "metadata": {
        "id": "51hKI-Gqo9zx"
      },
      "id": "51hKI-Gqo9zx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**see this**"
      ],
      "metadata": {
        "id": "W1soJit6qseY"
      },
      "id": "W1soJit6qseY"
    },
    {
      "cell_type": "code",
      "source": [
        "# sign function means signum function\n",
        "threshold = 0.1\n",
        "P_threshold = lambda x: np.sign(x) * np.max(np.abs(x) - threshold, 0)  # eqn 8\n",
        "P_threshold"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T82oCZmvn_aJ",
        "outputId": "3041775b-1cda-4836-b2af-a16584681262"
      },
      "id": "T82oCZmvn_aJ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.<lambda>(x)>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NIYHA6Ivq4Az"
      },
      "id": "NIYHA6Ivq4Az",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}